<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="November 5, 2018" />
  <title>BBD Seminar: Overview of tools and resources on campus and beyond: Savio, Box/bDrive, XSEDE</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">BBD Seminar: Overview of tools and resources on campus and beyond: Savio, Box/bDrive, XSEDE</h1>
  <p class="author">
November 5, 2018
  </p>
  <p class="date">Chris Paciorek (Berkeley Research Computing)</p>
</div>
<div id="introduction" class="slide section level1">
<h1>Introduction</h1>
<p>The goal this week is to (a) familiarize you with various resources and tools for computation and data storage available for researchers on campus and at the national level and (b) demonstrate the use of Spark for big data processing on the campus Savio cluster.</p>
<p>Two key take-home messages:</p>
<ul>
<li>If you feel like you're scaling back the questions you can answer because of computational limits, talk to us about larger-scale resources that may be available.</li>
<li>Consultants with Berkeley Research Computing and Research Data Management are here to help; see end of this document for contact info.</li>
</ul>
<p>The materials for this session are available using git at <a href="https://github.com/ucb-rit/computing-bbd-2018" class="uri">https://github.com/ucb-rit/computing-bbd-2018</a> or simply as a <a href="https://github.com/ucb-rit/computing-bbd-2018/archive/master.zip">zip file</a>. The HTML is also on bCourses.</p>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<p>This presentation will cover the following topics:</p>
<ul>
<li>Resources
<ul>
<li>Data storage</li>
<li>Savio resources</li>
<li>Getting access to Savio</li>
<li>XSEDE (NSF) resources</li>
<li>How to get help</li>
</ul></li>
<li>Spark
<ul>
<li>Overview of Map-Reduce</li>
<li>Spark overview</li>
<li>Spark demo using Python interface</li>
<li>Spark demo using R interface</li>
</ul></li>
</ul>
</div>
<div id="data-storage-for-large-datasets" class="slide section level1">
<h1>Data storage for large datasets</h1>
<ul>
<li>Box and Google Drive (i.e., bDrive) provide unlimited file storage.
<ul>
<li>Box provides <strong>unlimited</strong>, free, secured, and encrypted content storage of files with a maximum file size of 15 Gb to Berkeley affiliates.</li>
<li>bDrive provides <strong>unlimited</strong>, free, secured, and encrypted content storage of files with a maximum file size of 5 Tb to Berkeley affiliates.</li>
</ul></li>
<li>Savio scratch drive provides extensive short-term storage.</li>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing/brc-condo-storage-service-savio">Savio condo storage service</a> provides inexpensive long-term storage ($6200 for 42 TB for 5 years).</li>
<li>AWS and other cloud services have paid offerings.</li>
</ul>
</div>
<div id="data-storage-and-computation-for-confidential-data" class="slide section level1">
<h1>Data storage and computation for confidential data</h1>
<ul>
<li>UC Berkeley IST can set up secure virtual machines (VMs)</li>
<li>BRC has a template (developed through one of Alan Hubbard's projects) to analyze HIPAA-protected data on AWS</li>
<li>ongoing discussions about providing for secure data on Savio</li>
<li>BRC has a Windows-based virtual machine service, AEoD, that may be able to address secure data issue</li>
</ul>
<p>Research Data Management can help you explore options.</p>
</div>
<div id="system-capabilities-and-hardware" class="slide section level1">
<h1>System capabilities and hardware</h1>
<ul>
<li>Savio is a &gt;380-node, &gt;8000-core Linux cluster rated at &gt;300 peak teraFLOPS.</li>
<li>about 174 compute nodes provided by the institution for general access</li>
<li>about 211 compute nodes contributed by researchers in the Condo program</li>
</ul>
<p>Various pools (partitions) of nodes with specific capabilities</p>
<ul>
<li>24-core, 64 GB compute nodes (allocated per node)</li>
<li>large-memory nodes (128 GB and 512 GB)</li>
<li>GPU nodes</li>
<li>fast-CPU nodes allocated per core</li>
</ul>
<p>For more details, <a href="http://research-it.berkeley.edu/services/high-performance-computing/user-guide/savio-user-guide">see the <em>Hardware Configuration</em> section of this document</a>.</p>
</div>
<div id="getting-access-to-the-system---fca-and-condo" class="slide section level1">
<h1>Getting access to the system - FCA and condo</h1>
<ul>
<li>All regular Berkeley faculty can request 300,000 free service units (roughly core-hours) per year through the <a href="http://research-it.berkeley.edu/services/high-performance-computing/faculty-computing-allowance">Faculty Computing Allowance (FCA)</a></li>
<li>Researchers can also purchase nodes for their own priority access and gain access to the shared Savio infrastructure and to the ability to <em>burst</em> to additional nodes through the <a href="http://research-it.berkeley.edu/services/high-performance-computing/condo-cluster-program">condo cluster program</a></li>
<li>Instructors can request an <a href="http://research-it.berkeley.edu/programs/berkeley-research-computing/instructional-computing-allowance">Instructional Computing Allowance (ICA)</a>.</li>
</ul>
<p>Faculty/principal investigators can allow researchers working with them to get user accounts with access to the FCA or condo resources available to the faculty member.</p>
</div>
<div id="xsede-resources-overview" class="slide section level1">
<h1>XSEDE resources overview</h1>
<p>Some core resources we tend to refer researchers to:</p>
<ul>
<li>Jetstream
<ul>
<li>VM-based cloud resource</li>
<li>long running jobs/servers</li>
<li><em>possibly</em> can be used for secure data/compute</li>
<li>science gateways - web frontend for users to operate your computational tools</li>
</ul></li>
<li>Bridges
<ul>
<li>traditional HPC</li>
<li>Hadoop/Spark</li>
<li>large memory (up to 12 TB/node)</li>
<li>GPU nodes</li>
<li>web server nodes</li>
</ul></li>
<li>Comet
<ul>
<li>some traditional HPC</li>
<li>Singularity containers</li>
<li>GPU nodes</li>
<li>science gateways</li>
</ul></li>
</ul>
<p>Some additional resources:</p>
<ul>
<li>Stampede2
<ul>
<li>Intel Many Integrated Core architecture machine</li>
<li>KNL nodes with SKX to come</li>
</ul></li>
<li>XStream
<ul>
<li>GPU only machine</li>
</ul></li>
<li>Wrangler
<ul>
<li>data storage machine</li>
<li>persistent databases</li>
<li>Hadoop</li>
</ul></li>
</ul>
</div>
<div id="accessing-xsede" class="slide section level1">
<h1>Accessing XSEDE</h1>
<p>BRC consultants (see next slide) can help get you started.</p>
<p>XSEDE has:</p>
<ul>
<li>free initial exploration with limited core-hours (BRC can provide to you)</li>
<li>free startup allocations for benchmarking (easy to get with modest application)</li>
<li>free research allocations (more extensive application / more competitive)</li>
</ul>
</div>
<div id="getting-help" class="slide section level1">
<h1>Getting help</h1>
<ul>
<li>For questions about computing resources in general, including cloud computing:
<ul>
<li>brc@berkeley.edu</li>
<li>office hours: Tuesday 11-1, Thursday 9:30-11, and Friday 1:30-3 in the AIS (117 Dwinelle)</li>
</ul></li>
<li>For questions about data management (including HIPAA-protected data):
<ul>
<li>researchdata@berkeley.edu</li>
<li>office hours: Tuesday 11-1, Thursday 9:30-11, and Friday 1:30-3 in the AIS (117 Dwinelle)</li>
</ul></li>
<li>For technical issues and questions about using Savio:
<ul>
<li>brc-hpc-help@berkeley.edu</li>
</ul></li>
</ul>
</div>
</body>
</html>
